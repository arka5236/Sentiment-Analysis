# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19y035g38JqVRSXfTTIPR3lx3Me1SZ1UH

Data preprocessing
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# --------------------
# Step 1: Load the dataset with proper encoding and error handling.
# --------------------
df = pd.read_csv(
    "training.1600000.processed.noemoticon.csv",
    encoding="ISO-8859-1",  # Fix for UnicodeDecodeError
    header=None,
    quoting=3,
    on_bad_lines='skip'
)

# Keep only the first six columns and limit to the first 500 rows.
df = df.iloc[:500, :6]  # This selects only the first 500 rows.

# --------------------
# Step 2: Clean and filter the target column (column 0).
# --------------------
# The target values are stored with extra quotes (e.g. '"0"', '"4"')
# Remove the double quotes.
df[0] = df[0].str.replace('"', '', regex=False)

# Convert the target values from strings to numeric.
df[0] = pd.to_numeric(df[0], errors='coerce')

# Filter to keep only rows where the target (column 0) is either 0 or 4.
df = df[df[0].isin([0, 4])].copy()

# --------------------
# Step 3: Clean the text data (column 5).
# --------------------
# Convert text to lowercase.
df[5] = df[5].str.lower()

# Remove URLs and Twitter mentions.
df[5] = df[5].str.replace(r"http\S+|www\S+|@\S+", "", regex=True)

# Replace any non-letter characters with a space (this helps to preserve word boundaries).
df[5] = df[5].str.replace(r"[^a-zA-Z\s]", " ", regex=True)

# Collapse multiple spaces into one.
df[5] = df[5].str.replace(r"\s+", " ", regex=True)

# Remove leading and trailing whitespace.
df[5] = df[5].str.strip()

# Remove any rows where the text becomes empty after cleaning.
df = df[df[5] != ""]

# Optional: Check a few cleaned text samples.
print("Sample cleaned text:")
print(df[5].head())

# --------------------
# Step 4: Vectorize the text using TF-IDF.
# --------------------
# Use a token pattern that includes single-character tokens if needed.
vectorizer = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b", stop_words=None)
X_tfidf = vectorizer.fit_transform(df[5])

# --------------------
# Step 5: Convert to NumPy arrays.
# --------------------
# The TF-IDF data as a NumPy array:
X = X_tfidf.toarray()
# The target labels from column 0:
y = df[0].values

# --------------------
# Step 6: Split into training and testing sets.
# --------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

# Confirm the shapes of the split arrays.
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

df.head()

print(X)

print(y)

print(X_train)
print(X_train.shape)

print(X_test)
print(X_test.shape)

print(X_tfidf)

print(y_train)
print(y_train.shape)

print(y_test)
print(y_test.shape)

"""Quick EDA"""

import matplotlib.pyplot as plt
import seaborn as sns

"""Distribution of Sentimet Labels"""

sentiment_counts = df[0].value_counts().sort_index()
print("\nDistribution of Sentiment Labels (0: negative, 4: positive):")
print(sentiment_counts)

plt.figure(figsize=(6, 4))
sns.barplot(x=sentiment_counts.index.astype(str), y=sentiment_counts.values)
plt.title('Distribution of Sentiment Labels')
plt.xlabel('Sentiment (0: negative, 4: positive)')
plt.ylabel('Count')
plt.show()

"""Distribution of Text lengths.
Craete a new column for text lengths
"""

df['text_length'] = df[5].apply(len)
print("\nDescriptive statistics of text lengths:")
print(df['text_length'].describe())

plt.figure(figsize=(8, 4))
sns.histplot(df['text_length'], bins=30, kde=True)
plt.title('Distribution of Text Length (in characters)')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

"""Sum up the TF-IDF weights across all documents for each term"""

tfidf_sum = np.array(X_tfidf.sum(axis=0)).flatten()
print(tfidf_sum)
print(tfidf_sum.shape)

"""Get feature names from the vectorizer"""

vocab = vectorizer.get_feature_names_out()
print(vocab)

"""Create a DataFrame for words and their corresponding total TF-IDF weights."""

tfidf_df = pd.DataFrame({'word': vocab, 'total_tfidf': tfidf_sum})

"""Sort by weight in descending order to obtain the most influential words."""

tfidf_df = tfidf_df.sort_values(by='total_tfidf', ascending=False)
print("\nTop 10 words by total TF-IDF weight:")
print(tfidf_df.head(10))

plt.figure(figsize=(10, 6))
sns.barplot(x="total_tfidf", y="word", data=tfidf_df.head(10))
plt.title('Top 10 Words by Total TF-IDF Weight')
plt.xlabel('Total TF-IDF Weight')
plt.ylabel('Word')
plt.show()

"""Basic NTLK"""

import nltk
import matplotlib.pyplot as plt

"""Download necessary NTLK data files"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')


from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

"""Combine all cleaned text into one large string"""

all_text = " ".join(df[5].tolist())
print(all_text)

"""Tokenize the text into words"""

tokens = word_tokenize(all_text)

"""Convert tokens to lowercase to ensure consistency"""

tokens = [token.lower() for token in tokens]

"""Remove tokens that are not purely alhabetic"""

tokens = [token for token in tokens if token.isalpha()]

"""Load English stopwords"""

stop_words = set(stopwords.words('english'))

"""Filter out stopwords"""

filtered_tokens = [token for token in tokens if token not in stop_words]

"""Create a frequency distribution of the filtered tokens."""

freq_dist = nltk.FreqDist(filtered_tokens)

print("Total number of tokens:", len(tokens))
print("Total number of tokens after filtering stopwords and non-alphabetic tokens:", len(filtered_tokens))
print("\nTop 10 Most Common Tokens:")
print(freq_dist.most_common(10))

"""Plot the top 10 most common words"""

plt.figure(figsize=(10, 6))
freq_dist.plot(10, title='Top 10 Most Common Words')
plt.show()

"""Perform part of speech tagging on asample text"""

import nltk
import matplotlib.pyplot as plt

# ... (your existing code) ...

# Download necessary NTLK data files
nltk.download('punkt')
nltk.download('stopwords')
# Download with the language-specific resource identifier
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('punkt_tab')

# ... (rest of your existing code) ...

sample_text = df[5].iloc[0]
sample_tokens = word_tokenize(sample_text)
sample_tags = nltk.pos_tag(sample_tokens)

print("\nSample text:", sample_text)
print("POS tags:", sample_tags)

"""Vader Sentiment Scoring"""

from nltk.sentiment import SentimentIntensityAnalyzer

"""Download VADER lexicon"""

nltk.download('vader_lexicon')

"""Calculate VADER sentiment Scoring for each text."""

sia = SentimentIntensityAnalyzer()

df['vader_scores'] = df[5].apply(lambda text: sia.polarity_scores(text))

"""Extract the compound score for the summary measure"""

df['compound'] = df['vader_scores'].apply(lambda scores: scores['compound'])
print(df['compound'])

"""classify sentiment based on the compound score."""

#    compound >= 0.05  → Positive
#    compound <= -0.05 → Negative
#    else              → Neutral
df['vader_sentiment'] = df['compound'].apply(lambda score: 'positive' if score >= 0.05
                                               else ('negative' if score <= -0.05 else 'neutral'))

# Display the first few rows with the VADER results.
print(df[['vader_scores', 'compound', 'vader_sentiment']].head())

"""Plot VADER Results"""

import matplotlib.pyplot as plt
import seaborn as sns

# Bar plot of VADER sentiment categories
sentiment_counts = df['vader_sentiment'].value_counts()

plt.figure(figsize=(8, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette="viridis")
plt.title("Distribution of VADER Sentiment")
plt.xlabel("Sentiment Category")
plt.ylabel("Count")
plt.show()

# Histogram of VADER compound scores
plt.figure(figsize=(8, 6))
sns.histplot(df['compound'], bins=30, kde=True, color="blue")
plt.title("Distribution of VADER Compound Scores")
plt.xlabel("Compound Score")
plt.ylabel("Frequency")
plt.show()

"""Roberta Pretrained Model.Here,we use Hugging Face's Transformer library with a RoBERTa model that has been fine‑tuned for Twitter sentiment analysis."""

from transformers import pipeline

"""Initialize the RoBERTa Sentiment Analysis Pipeline"""

classifier = pipeline(
    'sentiment-analysis',
    model='cardiffnlp/twitter-roberta-base-sentiment',
    tokenizer='cardiffnlp/twitter-roberta-base-sentiment',
    truncation=True  # ensure texts longer than model limits are properly truncated
)

texts = df[5].tolist()
# Apply the classifier on all texts (batch processing helps with speed)
results = classifier(texts, truncation=True)

df['roberta_sentiment'] = [result['label'] for result in results]
df['roberta_score'] = [result['score'] for result in results]

"""Bar plot  of sentiment distribution"""

entiment_counts = df['roberta_sentiment'].value_counts()

plt.figure(figsize=(8, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='coolwarm')
plt.title("Distribution of RoBERTa Sentiment Predictions")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

# Histogram of the confidence scores from RoBERTa
plt.figure(figsize=(8, 6))
sns.histplot(df['roberta_score'], bins=30, kde=True, color='green')
plt.title("Distribution of RoBERTa Sentiment Confidence Scores")
plt.xlabel("Confidence Score")
plt.ylabel("Frequency")
plt.show()

"""Comapre between two models"""

df['vader_sentiment'] = df['vader_sentiment'].str.lower()
df['roberta_sentiment'] = df['roberta_sentiment'].str.lower()

"""Cross Tabulation (HeatMAp) of Sentiment Predictions"""

# Create a cross-tabulation counting the occurrence of each combination.
crosstab = pd.crosstab(df['vader_sentiment'], df['roberta_sentiment'])

# Also create a normalized version (percentage row-wise).
crosstab_norm = pd.crosstab(df['vader_sentiment'], df['roberta_sentiment'], normalize='index')

print("Cross Tabulation (counts):")
print(crosstab)

print("\nCross Tabulation (row-normalized):")
print(crosstab_norm)

plt.figure(figsize=(8, 6))
sns.heatmap(crosstab_norm, annot=True, cmap="YlGnBu", fmt=".2f")
plt.title("Normalized Cross-Tabulation: VADER vs. RoBERTa Sentiment")
plt.xlabel("RoBERTa Sentiment")
plt.ylabel("VADER Sentiment")
plt.show()

sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}
if 'roberta_numeric' not in df.columns:
    df['roberta_numeric'] = df['roberta_sentiment'].map(sentiment_mapping)

plot_cols = ["compound", "roberta_score", "roberta_numeric"]

# Create the pairplot. We add hue using 'roberta_sentiment' for a richer visualization.
sns.pairplot(
    df[plot_cols + ["roberta_sentiment"]],
    hue="roberta_sentiment",
    diag_kind="kde",
    height=3
)
plt.suptitle("Pairplot: VADER and RoBERTa Sentiment Metrics", y=1.02)
plt.show()